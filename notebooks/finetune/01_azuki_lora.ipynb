{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# examples\n",
    "image under ./images folder are genereated by our fintuned model, using sdxl as base model \n",
    "\n",
    "\n",
    "# 0. setup GPU server\n",
    "we suggest, at least you shoud have 16G gpu, and 64G cpu \n",
    "\n",
    "# 1. install requerments \n",
    "pip install torch torchvision transformers pillow requests clip-interrogator pert\n",
    "pip install accelerate peft deepspeed\n",
    "\n",
    "# 2.  prepare annotation data \n",
    "using clip or clip to annotate data \n",
    "annotate_image('images)\n",
    "\n",
    "# 3. prepare the metadata.jsonl\n",
    " Generate metadata.jsonl file containing image paths, captions, and style information\n",
    " generate_metadata('images', 'images', \"Azuki, Azuki Styple, Azuki art, azuki\")\n",
    "\n",
    "# 4.  train your lora model with bash comment\n",
    "#!/bin/bash\n",
    "accelerate launch \\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --num_processes=1 \\\n",
    "  --num_machines=1 \\\n",
    "  --machine_rank=0 \\\n",
    "  --dynamo_backend=\"no\" \\\n",
    "  train_text_to_image_lora_sdxl.py \\\n",
    "  --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-xl-base-1.0\" \\\n",
    "  --train_data_dir=\"./images\" \\\n",
    "  --resolution=768 \\\n",
    "  --output_dir=\"./images\" \\\n",
    "  --train_batch_size=2 \\\n",
    "  --gradient_accumulation_steps=4 \\\n",
    "  --learning_rate=1e-4 \\\n",
    "  --num_train_epochs=10 \\\n",
    "  --rank=8 \\\n",
    "  --train_text_encoder \\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --enable_xformers_memory_efficient_attention \\\n",
    "  --validation_epochs=5 \\\n",
    "  --checkpointing_steps=100\n",
    "\n",
    "# 5. generate azuki style NFT avators\n",
    "prompt = \"sunshine boy, cigarette in mouth, sword in hand, Azuki style, high quality, vivid colors, clean background, single color background\" # 你可以修改为任何你想要的提示词\n",
    "\n",
    "generate_images(\n",
    "    prompt=prompt,\n",
    "    output_dir=\"output\",\n",
    "    num_images=1,\n",
    "    width=720,\n",
    "    height=720,\n",
    "    pretrained_model_path=\"stabilityai/stable-diffusion-xl-base-1.0\", \n",
    "    lora_path=\"images/pytorch_lora_weights.safetensors\",\n",
    "    lora_scale=0.7,\n",
    "    seed=42 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from clip_interrogator import Config, Interrogator\n",
    "import sys\n",
    "\n",
    "def annotate_image(train_data_dir):\n",
    "    # Configure paths\n",
    "    output_caption_dir = f\"{train_data_dir}.captions\"  # Directory to store captions\n",
    "    os.makedirs(output_caption_dir, exist_ok=True)\n",
    "\n",
    "    # Choose between BLIP or CLIP Interrogator\n",
    "    USE_BLIP = False  # True = use BLIP, False = use CLIP Interrogator\n",
    "\n",
    "    # ========== Method 1: BLIP Auto Caption Generation ==========\n",
    "    if USE_BLIP:\n",
    "        # Load BLIP model\n",
    "        processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "        model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").to(\"cuda\")\n",
    "\n",
    "        def generate_caption_blip(image_path):\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            inputs = processor(image, return_tensors=\"pt\").to(\"cuda\")\n",
    "            caption_ids = model.generate(**inputs)\n",
    "            caption = processor.batch_decode(caption_ids, skip_special_tokens=True)[0]\n",
    "            return caption\n",
    "\n",
    "    # ========== Method 2: CLIP Interrogator Caption Generation ==========\n",
    "    else:\n",
    "        ci_config = Config(device=\"cuda\")\n",
    "        ci = Interrogator(ci_config)\n",
    "\n",
    "        def generate_caption_clip(image_path):\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            caption = ci.interrogate(image)\n",
    "            return caption\n",
    "\n",
    "    # Loop through all images and generate captions\n",
    "    for filename in os.listdir(train_data_dir):\n",
    "        if filename.endswith((\".png\", \".jpg\", \".jpeg\", \".webp\")):\n",
    "            image_path = os.path.join(train_data_dir, filename)\n",
    "            caption = generate_caption_blip(image_path) if USE_BLIP else generate_caption_clip(image_path)\n",
    "\n",
    "            # Generate corresponding txt file\n",
    "            txt_path = os.path.join(output_caption_dir, f\"{os.path.splitext(filename)[0]}.txt\")\n",
    "            with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(caption)\n",
    "\n",
    "            print(f\"✅ Generated Caption: {filename} -> {caption}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "\n",
    "def generate_metadata(image_dir, captions_path, style_desc):\n",
    "    \"\"\"\n",
    "    Generate metadata.jsonl file containing image paths, captions, and style information\n",
    "\n",
    "    Args:\n",
    "        image_dir: Path to image directory\n",
    "        captions_path: Path to captions file or directory\n",
    "        style_desc: Additional style description text\n",
    "    \"\"\"\n",
    "    # Ensure input directory exists\n",
    "    if not os.path.exists(image_dir):\n",
    "        print(f\"Error: Image directory '{image_dir}' does not exist\")\n",
    "        return\n",
    "\n",
    "    # Get all image files\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.gif', '.webp']\n",
    "    image_files = []\n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(image_dir, f\"*{ext}\")))\n",
    "        image_files.extend(glob.glob(os.path.join(image_dir, f\"*{ext.upper()}\")))\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"Warning: No image files found in '{image_dir}'\")\n",
    "        return\n",
    "\n",
    "    # Process caption files\n",
    "    captions = {}\n",
    "    if os.path.isfile(captions_path):\n",
    "        # If captions_path is a file, try to read captions from it\n",
    "        try:\n",
    "            with open(captions_path, 'r', encoding='utf-8') as f:\n",
    "                # Try to read different caption file formats\n",
    "                if captions_path.endswith('.json') or captions_path.endswith('.jsonl'):\n",
    "                    for line in f:\n",
    "                        try:\n",
    "                            data = json.loads(line.strip())\n",
    "                            if 'file' in data and 'caption' in data:\n",
    "                                captions[data['file']] = data['caption']\n",
    "                        except json.JSONDecodeError:\n",
    "                            continue\n",
    "                else:\n",
    "                    # Assume file format is: filename|caption per line\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split('|', 1)\n",
    "                        if len(parts) == 2:\n",
    "                            captions[parts[0]] = parts[1]\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading caption file: {e}\")\n",
    "    elif os.path.isdir(captions_path):\n",
    "        # If captions_path is a directory, try to find corresponding caption file for each image\n",
    "        for image_file in image_files:\n",
    "            base_name = os.path.splitext(os.path.basename(image_file))[0]\n",
    "            caption_file = os.path.join(captions_path, f\"{base_name}.txt\")\n",
    "            if os.path.exists(caption_file):\n",
    "                try:\n",
    "                    with open(caption_file, 'r', encoding='utf-8') as f:\n",
    "                        captions[os.path.basename(image_file)] = f.read().strip()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {caption_file}: {e}\")\n",
    "\n",
    "    # Create metadata.jsonl file\n",
    "    parent_dir = os.path.dirname(os.path.abspath(image_dir))\n",
    "    output_file = os.path.join(parent_dir, \"metadata.jsonl\")\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for image_file in sorted(image_files):\n",
    "            file_name = os.path.basename(image_file)\n",
    "            rel_path = os.path.relpath(image_file, parent_dir)\n",
    "\n",
    "            # Get caption for image, use filename if no caption exists\n",
    "            caption = captions.get(file_name, file_name)\n",
    "\n",
    "            # Add style description to caption if provided\n",
    "            if style_desc:\n",
    "                full_caption = f\"{caption}, {style_desc}\"\n",
    "            else:\n",
    "                full_caption = caption\n",
    "\n",
    "            # Create metadata entry\n",
    "            metadata = {\n",
    "                \"file\": rel_path,\n",
    "                \"caption\": full_caption\n",
    "            }\n",
    "\n",
    "            # Write JSON line\n",
    "            f.write(json.dumps(metadata, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    print(f\"Successfully generated metadata.jsonl file with {len(image_files)} image entries\")\n",
    "    print(f\"Saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "annotate_image('images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    " generate_metadata('images', 'images', \"Azuki, Azuki Styple, Azuki art, azuki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "from diffusers import StableDiffusionXLPipeline, DPMSolverMultistepScheduler, UNet2DConditionModel, AutoencoderKL\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "def generate_images(prompt, output_dir=\"output\", num_images=1, width=720, height=720,\n",
    "                    pretrained_model_path=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "                    lora_path=\"images/pytorch_lora_weights.safetensors\",\n",
    "                    lora_scale=0.7, seed=None):\n",
    "    \"\"\"\n",
    "    Generate images with and without LoRA\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt for image generation\n",
    "        output_dir (str): Output directory\n",
    "        num_images (int): Number of images to generate for each type\n",
    "        width (int): Image width\n",
    "        height (int): Image height\n",
    "        pretrained_model_path (str): Path to pretrained model\n",
    "        lora_path (str): Path to LoRA weights file\n",
    "        lora_scale (float): LoRA intensity\n",
    "        seed (int): Random seed, set to None for random seed\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    # Load base SDXL model\n",
    "    print(\"Loading base SDXL model...\")\n",
    "\n",
    "    # Use loading method similar to train.py\n",
    "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "        pretrained_model_path,\n",
    "        torch_dtype=torch.float16,\n",
    "        variant=\"fp16\",\n",
    "        use_safetensors=True\n",
    "    )\n",
    "\n",
    "    # Optimize inference speed\n",
    "    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "    pipe.to(\"cuda\")\n",
    "    pipe.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "    # First generate original images (without LoRA)\n",
    "    print(f\"Generating {num_images} images without LoRA...\")\n",
    "    for i in range(num_images):\n",
    "        current_seed = torch.initial_seed() if seed is None else seed + i\n",
    "        generator = torch.Generator(device=\"cuda\").manual_seed(current_seed)\n",
    "\n",
    "        image = pipe(\n",
    "            prompt=prompt,\n",
    "            width=width,\n",
    "            height=height,\n",
    "            num_inference_steps=30,\n",
    "            generator=generator\n",
    "        ).images[0]\n",
    "\n",
    "        # Save image\n",
    "        image_path = os.path.join(output_dir, f\"no_lora_seed_{current_seed}.png\")\n",
    "        image.save(image_path)\n",
    "        print(f\"Saved image to: {image_path}\")\n",
    "\n",
    "    # Load LoRA weights\n",
    "    print(f\"Loading LoRA weights: {lora_path}\")\n",
    "    if os.path.exists(lora_path):\n",
    "        try:\n",
    "            # Use diffusers' LoRA loading method\n",
    "            pipe.load_lora_weights(lora_path)\n",
    "            pipe.fuse_lora(lora_scale=lora_scale)\n",
    "            lora_loaded = True\n",
    "        except Exception as e:\n",
    "            print(f\"LoRA loading error: {e}\")\n",
    "            print(\"LoRA weights are not compatible with SDXL model architecture. This usually happens when trying to use LoRA trained for SD1.5 with SDXL model.\")\n",
    "            print(\"Continuing with base model generation...\")\n",
    "            lora_loaded = False\n",
    "    else:\n",
    "        print(f\"Error: LoRA file {lora_path} does not exist!\")\n",
    "        lora_loaded = False\n",
    "\n",
    "    # Generate images with LoRA\n",
    "    if lora_loaded:\n",
    "        print(f\"Generating {num_images} images with LoRA...\")\n",
    "        for i in range(num_images):\n",
    "            current_seed = torch.initial_seed() if seed is None else seed + i\n",
    "            generator = torch.Generator(device=\"cuda\").manual_seed(current_seed)\n",
    "\n",
    "            image = pipe(\n",
    "                prompt=prompt,\n",
    "                width=width,\n",
    "                height=height,\n",
    "                num_inference_steps=30,\n",
    "                generator=generator\n",
    "            ).images[0]\n",
    "\n",
    "            # Save image\n",
    "            image_path = os.path.join(output_dir, f\"with_lora_seed_{current_seed}.png\")\n",
    "            image.save(image_path)\n",
    "            print(f\"Saved image to: {image_path}\")\n",
    "\n",
    "        # Unload LoRA weights\n",
    "        pipe.unfuse_lora()\n",
    "    else:\n",
    "        print(\"Skipping LoRA image generation due to failed LoRA loading.\")\n",
    "\n",
    "    print(\"Image generation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"sunshine boy, cigarette in mouth, sword in hand, Azuki style, high quality, vivid colors, clean background, single color background\" # 你可以修改为任何你想要的提示词\n",
    "\n",
    "generate_images(\n",
    "    prompt=prompt,\n",
    "    output_dir=\"output\",\n",
    "    num_images=1,\n",
    "    width=720,\n",
    "    height=720,\n",
    "    pretrained_model_path=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    lora_path=\"images/pytorch_lora_weights.safetensors\",\n",
    "    lora_scale=0.7,\n",
    "    seed=42\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
