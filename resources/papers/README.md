# Recommended Papers on LLM Evaluation

This document collects some of the most valuable and representative papers and resources on the evaluation of large language models (LLMs). These works cover benchmark design, evaluation methodologies, leaderboard systems, and comprehensive surveys. Each entry includes a direct link and a brief annotation for quick reference.

---

- [MEASURING MASSIVE MULTITASK LANGUAGE UNDERSTANDING](https://arxiv.org/pdf/2009.03300)
Presents the MMLU benchmark, a widely used test for evaluating multitask language understanding in LLMs.

- [A Survey on Evaluation of Large Language Models](https://arxiv.org/pdf/2307.03109)
A comprehensive survey summarizing current evaluation methods and challenges for LLMs.

- [Evaluating Large Language Models: A Comprehensive Survey](https://arxiv.org/pdf/2310.19736)
Another in-depth survey covering benchmarks, metrics, and open problems in LLM evaluation.

- [Chatbot Arena: Benchmarking LLMs in the Wild with Elo Ratings | LMSYS Org](https://lmsys.org/blog/2023-05-03-arena/)
Introduces the Chatbot Arena, an interactive leaderboard using Elo ratings for LLM comparison based on real user feedback.

- [Open LLM Leaderboard - a Hugging Face Space by open-llm-leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)
Describes the Hugging Face Open LLM Leaderboard, an open and transparent platform for LLM benchmarking.

- [Holistic Evaluation of Language Models (HELM)](https://crfm.stanford.edu/helm/)
Presents the HELM framework, which evaluates LLMs across multiple dimensions such as accuracy, robustness, and fairness.

- [OpenCompass LLM Evaluation Platform Website](https://opencompass.org.cn/)
Official website for OpenCompass, a multimodal evaluation platform for large models.

- [AlpacaEval Leaderboard for Instruction-Following Models](https://tatsu-lab.github.io/alpaca_eval/)
Describes AlpacaEval, an automated tool for comparing LLMs using model-based judgments.

- [GenAI-Bench: A Holistic Benchmark for Text-to-Visual Generation](https://openreview.net/forum?id=hJm7qnW3ym)
Presents a benchmark for evaluating text-to-visual generation capabilities of generative AI models.

- [Google BIG-Bench Collaborative Benchmark Repository](https://github.com/google/BIG-bench)
Repository for BIG-Bench, a collaborative benchmark suite with over 200 tasks for LLMs.

- [OpenAI Evals Framework for LLM Evaluation](https://github.com/openai/evals)
OpenAI's official framework for evaluating LLMs, supporting custom and standard benchmarks.

- [EleutherAI LM Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness)
A modular evaluation toolkit widely used as the backend for many LLM leaderboards and benchmarks.

- [xbench: Tracking Agents Productivity, Scaling with Profession-Aligned Real-World Evaluations](https://xbench.org/files/xbench_profession_v2.4.pdf)
Presents xbench, a benchmark focused on real-world, profession-aligned evaluation of AI agents.

